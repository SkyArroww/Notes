# **Animatable Single-Image 3D Avatar Reconstruction**

### 任务内容：

从单图人体输入图实现可驱动的人体重建，同时要求重建出的人体具有高清晰度的纹理材质。 现有的单图人体重建大多关注于更精确的几何表面重建，通常利用Multi-View Diffusion模型幻想出两个视角或四个视角的图像，然后通过“脑补”的图像估计出人体的SDF或隐式表面，最后还原出高细节的人体Mesh。 但在实践中发现，这种pipeline构建出的Mesh难以实现高精度的骨骼绑定（Rigging），更有甚者无法实现骨骼绑定，从而导致现有单张图重建方法在Animation层面上存在较大困难与挑战。

### 参考思路：

首先需要参考CharacterGen设计并训练一个能生成多个视角A pose的diffusion模型，建议采用视频生成模型不要采用图像diffusion。diffusion模型必须以A pose形式生成多视角图像，建议图像数量能达到8个视角，由于遮挡的广泛存在视角越多遮挡对最终结果影响越小。三维重建可以采用两种技术路线，可以采用3DGS配合SMPL的高斯重建路线，此路线可以参考https://github.com/lizhe00/AnimatableGaussians ,采用多视角重建手段直接从“脑补”的多视角图像中得到3DGS模型，生成后可以利用SMPL直接驱动模型。第二条路线是采用SDF模型重建出符号场后建立模型的mesh模型，然后可以利用A pose的优势直接自动化骨骼绑定，即可实行可以驱动的3D模型重建，这种思路核心问题在于纹理映射问题，可以采用直接利用diffusion模型inpainting一个texture，然后贴上模型，这个过程中会存在一定程度的映射误差问题，最终效果呈现为纹理与几何错位。

参考baseline: 

1. [SIFU: Side-view Conditioned Implicit Function for Real-world Usable Clothed Human Reconstruction](https://arxiv.org/abs/2312.06704)
2. [SiTH: Single-view Textured Human Reconstruction with Image-Conditioned Diffusion](https://arxiv.org/abs/2311.15855)
3. [Semantic Human Mesh Reconstruction with Textures](https://arxiv.org/abs/2403.02561)
4. [Edify 3D: Scalable High-Quality 3D Asset Generation](https://arxiv.org/abs/2411.07135)
5. [CharacterGen: Efficient 3D Character Generation from Single Images with Multi-View Pose Canonicalization](https://arxiv.org/abs/2402.17214)
6. [V3D: Video Diffusion Models are Effective 3D Generators](https://arxiv.org/abs/2403.06738)

### 项目目标及成果形式：

- [Top1] 论文发表为最佳项目目标
- [Top2] 重要竞赛得奖
- [Top3] 具有影响力(star>1000)的开源项目

### 数据集：

A. https://github.com/ytrock/THuman2.0-Dataset

B. https://github.com/GAP-LAB-CUHK-SZ/MVHumanNet
